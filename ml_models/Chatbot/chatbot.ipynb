{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import chainlit as cl\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Volumes/Transcend/Development/Depresio/backend/ml_models/Chatbot/Knowledge/docs\"\n",
    "DB_FAISS_PATH = \"/Volumes/Transcend/Development/Depresio/backend/ml_models/Chatbot/Knowledge/vectorstore/df_faiss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_vector_db():\n",
    "#     if not os.path.exists(DB_FAISS_PATH):\n",
    "#         print(\"Vector DB already exists\")\n",
    "#     else:\n",
    "#         loader = DirectoryLoader(\"DATA_PATH\", glob=\"*.txt\")\n",
    "#         document = loader.load()\n",
    "\n",
    "\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(\n",
    "#             chunk_size = 500, \n",
    "#             chunk_overlap = 50)\n",
    "\n",
    "#         texts = text_splitter.split_documents(document)\n",
    "\n",
    "#         embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#                                            model_kwargs={'device': 'mps'})\n",
    "        \n",
    "#         db = FAISS.from_documents(texts,embeddings)\n",
    "\n",
    "#         db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "#         print('vector db created. ')\n",
    "\n",
    "# create_vector_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-09 22:15:49 - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 15.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector db created. \n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(DATA_PATH, glob=\"*.txt\")\n",
    "document = loader.load()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = 500, \n",
    "            chunk_overlap = 50)\n",
    "\n",
    "texts = text_splitter.split_documents(document)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                           model_kwargs={'device': 'mps'})\n",
    "        \n",
    "db = FAISS.from_documents(texts,embeddings)\n",
    "\n",
    "db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "print('vector db created. ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt_template = \"\"\"Use the following piece of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and noting else.\n",
    "Helpful answers:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_prompt():\n",
    "    prompt = PromptTemplate(custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_qa_chain(llm, prompt, db):\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type='stuff',\n",
    "        retriever = db.as_retriever(search_kwargs={'k':2}),\n",
    "        return_source_documents = True,\n",
    "        chain_type_kwargs={'prompt': prompt}\n",
    "    )\n",
    "\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_llm():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
